{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In which folder am I?"
      ],
      "metadata": {
        "id": "hoMYXFIeQnr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f24CKYfRC-g",
        "outputId": "03dc2551-76a6-40c6-d3c4-d585ceb3795b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6vB8DHFRx7p",
        "outputId": "31a14799-da6e-44ab-ad04-a7ac837f9b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chess\n",
            "  Downloading chess-1.10.0-py3-none-any.whl (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/154.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chess\n",
            "Successfully installed chess-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b--KuJwrU4WD",
        "outputId": "f1787696-d9c1-4ad0-d6ef-53a6fe919782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan  2 18:15:29 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8               8W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import chess\n",
        "import os.path as path\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "# load the dataset, split into input (X) and output (y) variables\n",
        "# BE CAREFUL the split must be changed if we use a different representation of who is winning cpawn vs, [1,0]\n",
        "one_hot_mapping = {\n",
        "    0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # Empty\n",
        "    1: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # White Pawn\n",
        "    3: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],  # White Knight/Bishop\n",
        "    5: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],  # White Rook\n",
        "    10: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],  # White Queen\n",
        "    1000: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],  # White King\n",
        "    -1: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],  # Black Pawn\n",
        "    -3: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],  # Black Knight/Bishop\n",
        "    -5: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],  # Black Rook\n",
        "    -10: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],  # Black Queen\n",
        "    -1000: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  # Black King\n",
        "}\n",
        "\n",
        "train = True\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "modelName = \"models/one_hot_from_colab.pt\"\n",
        "def transformSingleBoardToOneHot(board):\n",
        "    newBoardRepresentation = np.array([board[0]]) # First entry is whose turn it is\n",
        "    for field in board[1:]:\n",
        "        newBoardRepresentation = np.append(newBoardRepresentation, one_hot_mapping[field])\n",
        "\n",
        "    return newBoardRepresentation\n",
        "def transformBoardsCsvToOneHot(boards):\n",
        "    oneHotEncodedValuesFileName = \"data/p2_one_hot_encoded.npy\"\n",
        "    if path.isfile(oneHotEncodedValuesFileName):\n",
        "        with open(oneHotEncodedValuesFileName, 'rb') as f:\n",
        "            return np.load(f)\n",
        "    newBoardsRepresentation = np.array([])\n",
        "    for board in boards:\n",
        "        newBoardRepresentation = transformSingleBoardToOneHot(board)\n",
        "        newBoardsRepresentation = np.append(newBoardsRepresentation, newBoardRepresentation)\n",
        "\n",
        "    newBoardsRepresentation = newBoardsRepresentation.reshape(len(boards), 641) #641 = 1+64*10 because one hot vector has 10 elements\n",
        "    with open(oneHotEncodedValuesFileName,\"wb\") as f:\n",
        "        np.save(f, newBoardsRepresentation)\n",
        "    return newBoardsRepresentation\n",
        "\n",
        "\n",
        "if train:\n",
        "    print('Using device:', device)\n",
        "    dataset = np.loadtxt('data/p2.csv', delimiter=',') # use same dataset because no reason to change\n",
        "    X = dataset[:, :65]\n",
        "    y = dataset[:, 65:]\n",
        "    X = transformBoardsCsvToOneHot(X)\n",
        "    # Convert to tensors\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    y_one_hot = torch.tensor(y, dtype=torch.float32)\n",
        "    y_class_indices = torch.argmax(y_one_hot, dim=1) #pytorch needs integer values for nn.CrossEntropyLoss\n",
        "\n",
        "    # define model\n",
        "    noOfCpawnValues = 5\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(1 + 64 * noOfCpawnValues * 2, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, 3))\n",
        "\n",
        "    # load model:\n",
        "    if path.isfile(modelName):\n",
        "        model = torch.load(modelName, map_location=device)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss() # TODO still not sure? habs auch nochmal gegoogelt\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam optimizer\n",
        "    batch_size = 100\n",
        "    dataset = TensorDataset(X, y_class_indices)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    n_epochs = 2000\n",
        "    for epoch in range(n_epochs):\n",
        "        avg_loss = 0\n",
        "        amount = 0\n",
        "        for Xbatch, ybatch in dataloader:\n",
        "            Xbatch, ybatch = Xbatch.to(device), ybatch.to(device)\n",
        "            y_pred = model(Xbatch)\n",
        "            loss = loss_fn(y_pred, ybatch)\n",
        "            avg_loss += loss.item()\n",
        "            amount += 1\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        # Print loss after each epoch\n",
        "        print(f'Finished epoch {epoch}, latest loss {str(avg_loss/amount)}')\n",
        "\n",
        "    torch.save(model, modelName)\n",
        "    model = torch.load(modelName)\n",
        "    model.eval()\n",
        "else:\n",
        "    model = torch.load(modelName)\n",
        "\n",
        "whiteWinning = \"3k4/8/3K4/8/8/8/8/Q7 w - - 0 2\"\n",
        "blackWinning = \"8/2k5/8/2q5/8/8/7R/7K w - - 0 1\"\n",
        "\n",
        "def convertPositionToString(fen):\n",
        "    piece_values = {'p': 1, 'r': 5, 'n': 3, 'b': 3, 'q': 10, 'k': 1000}\n",
        "    board = chess.Board(fen)\n",
        "    board = str(board)\n",
        "    lines = board.split('\\n')\n",
        "\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        for char in line.split(' '):\n",
        "            char = char.strip()\n",
        "            if char.lower() in piece_values:\n",
        "                value = piece_values[char.lower()]\n",
        "                result.append(str(value) if char.islower() else str(-value))\n",
        "            else:\n",
        "                result.append('0')\n",
        "\n",
        "    return ','.join(result)\n",
        "\n",
        "def testFenPosition(fen):\n",
        "    test = (\"1,\"+ convertPositionToString(fen)).split(\",\")\n",
        "    test = [int(t) for t in test]\n",
        "    test = transformSingleBoardToOneHot(test)\n",
        "    test = torch.tensor(test, dtype=torch.float32)\n",
        "    test.to(device)\n",
        "    predictions = model(test)\n",
        "    print(predictions)\n",
        "\n",
        "with torch.no_grad(): # uses less memory, random optimization when doing inference\n",
        "    testFenPosition(whiteWinning)\n",
        "    testFenPosition(blackWinning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6AW8dOgRNU5",
        "outputId": "62f819b2-ad34-4971-d1bf-e6172cebc962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished epoch 786, latest loss 4.023640174145674e-05\n",
            "Finished epoch 787, latest loss 4.017982654800562e-05\n",
            "Finished epoch 788, latest loss 4.005523319420399e-05\n",
            "Finished epoch 789, latest loss 3.988586524935803e-05\n",
            "Finished epoch 790, latest loss 3.979137685290104e-05\n",
            "Finished epoch 791, latest loss 0.0002546962872875338\n",
            "Finished epoch 792, latest loss 0.0003802794073670151\n"
          ]
        }
      ]
    }
  ]
}