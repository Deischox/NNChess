How could we use reinforcemeant learning?

General/Terminology:
    - State and actions are input to NN, rewards and best actions are output
    - Learn by trial/error with environment to achieve a high score on a reward function
        --> Take some action
        --> Environment decides if action was good/bad
        --> Learn which actions are (un)desirable
    - Markov Decision Process (MDP) used in RL consists of 5 parts:
        - Agent (our chess bot) --> Plays against environment
        - Environment that we interact with --> Chess board and its rules to move and our goal (win?)
        - Current state of the world --> Chess board
            --> Maybe we need to consider the history of how we got there (not neccessarily though)
        - Action we can take --> Legal moves for all pieces
        - Reward --> Agent receives this to decide if action is good or bad
            --> Agent tries to maximize rewards throughout game
                --> We can give reward after each action or after an episode (batch/game)
            --> Shouldnt focus on immediate reward only but also on subsequent values
    - How does agent pick action?
        - Return --> Total reward over all time steps (we favor total rewards rather than individual rewards)
            - Discounted Rewards: We have a factor y = [0,1] which we apply to rewards that are after our
                currently considered action. (r₀ is current action and r₁ the next move ...)
                --> Return = r₀ +γ r₁ + γ² r₂
        - Policy --> Strategy followed to pick an action
            - Possibilities:
                - Random action
                - Action that has highest known reward for state(evaluation from stockfish?)
                - Force exploration of new path
                - Play safe and dont want to get negative reward
        - Value --> Expected return
            - Given a state, what return can we expect on average?
            - State value (V-Value): If we apply policy x to this state, what return can we expect?
            - State Action Value (Q-Value): Taking a random action and then follow the policy. What return can we expect?
            - Optimal policy is the one that has highest value
                --> This is the problem we re solving with RL. So we want to find the policy that gives us the most rewards.
    - Prediction vs control models
        - Prediction is given a policy, give the value function
            --> Can produce exact outcome of every state and action interaction.
            --> Concerning chess that would be also integrating strategies into the agent from the beginning
        - Control is finding the best policy
            --> Concerning chess this means it just tries random stuff and see how it works basically
    - Bellman Equation for Return
        - Always have immediate return + discounted return from action thereafter
        - Can be calculated for state value and state action value
    - How to update estimated State Action Value (one way, called TD):
        --> Error = Observed reward - + return from next state - Return from current state
        --> Error = R1 + yQ(S_3, a_3) - Q(S_1, a_1)
            - R_1 is reward for first action, S_3 is state 3, a_3 is action 3
    - Flow to update is
        - Start with empty initial state (0 value for each state/action-pair)
        - Agent takes action from current state, use epsilon-greedy for value-based
        - Agent gets new state data after opponent also moved a piece
        - Update estimates using observed reward to get more accurate estimates
    - Exploration vs Exploitation
        - Exploration --> Model tries different actions and observes rewards
        - Exploitation --> Model is trained and can pick action that yields best return
    - Q-Function --> Function to maximize (reward function)
        - Need to add mechanism to encourage exploration
            - Epsilon Greedy
                --> More exploration in earlier stages of training, more exploitation in later stages of training
            - Boltzmann Exploration
        - When our action has impact on the state of the game (as in chess) we have to take that into consideration.
          We then have an immediate reward and a reward we expect for later moves
            - This requires Temporal Difference Learning --> Looks into the future (similar to minimax with depth?)
        - Another approach is Monte Carlo --> Playing entire game before we update anything
            --> Advantages: Efficient propagation in batch than after each move
        - Can have discrete vs continuous problem (chess is discrete cuz "limited" number of states and moves)
    - Deep Q Network (DQN)
        - Architecture
            - Has two neural nets:
                - Q Network --> This is our agent we train for optimal state action value
                    - "Standard NN" an can be RNN if state is text (which would be the case)
                - Target --> Identical to Q network
            - Experience Replay: Interacts with env to generate data to train Q network
        - Workflow:
            1. Exp Replay selects & performs random action from current state, gets reward and new state
                --> saves this as training sample (current state, action, reward, new state)
            2. Q Network predicts Q-value
                --> All prior Exp Replay observations are training data
                --> We take batch from this (mix of old&new data)
                --> Q-Network takes state and action from each sample and predicts Q value for each eaction (Predicted Q Value)
                --> Target net takes next state and predicts best Q value out of all actions that can be taken (Target Q Value)
                    --> Next state is from the same example that we feed into Q net but "next state" attribute
            3. Compute loss and train net
                - Predicted Q Value, Target Q Value and observed reward is used to compute loss
                    to train Q network, target network is not trained
        - Why need Experience Replay?
            - So we can have shuffled training data
                --> If we used 5 moves that were done in a row we only train on that part of the game
                    --> Weights are updated just using this sequence
       - Why need second net (target net)?
            - Ensures target q values remain stable for a short period
            - After some steps the learned weights from q network are copied to target net


Application to chess:
    - To maximize: Stockfish evaluation of position after x turns
        --> Goal is to get best position after x turns, which could be evaluated by stockfish then
    - Try to "survive" x turns against y elo?
    - Reduced scope: Only from starting board
    - Possible reward functions
        --> Get positive or neutral cpawn value within next two turns+
        --> Maximize stockfish evaluation to win (or not to lose?)
        --> Win/lose
    - Prediction vs Control
        - Prediction --> Assess strength of position
        - Control --> Find Optimal strategy to get most rewards. So we want to find a strategy to win the game most likely
            --> Find optimal policy "policy-based" (idk what that means yet) or state based (can use greedy-epsilon)

    -Q Function
        - Actions are all moves that could possibly be done
            - Maybe limit these? only pawns and queen ie
        - States are all states that could possibly be arranged (oof)
            - Limit for possiblites after first 5 turns ie
    - Enemy for beginning might not be stockfish but making random moves maybe?
        - Otherwise we wouldnt get rewards for the actions we do because we always lose?

Caveats:
    - Might need to limit number of turns
    - Only one player side (white ie)
    - Actions we can do vary for each state
    - If reward function rewards wins then we have to wait a long time for feedback
        --> Use stockfish evaluation for instant feedback
        --> also says how good/bad a move was and not only that is was good/bad
Hyperparamters
    - Reward function
        - Discount value (y)
    - Policy
    - Starting position of board
    - Improve estimates
        - Frequency - number of steps before update (episode, one step, n step)
            --> Episode might not be suitable as it might take long to finish a game (we could limit turns though)
        - Depth - number of backwards steps to propagate update (episode, one step, n step)
        - Formula that is used to compute updated estimate (TD error ie)




Resources used:
    - https://blog.insightdatascience.com/reinforcement-learning-from-scratch-819b65f074d8
    - https://towardsdatascience.com/reinforcement-learning-made-simple-part-1-intro-to-basic-concepts-and-terminology-1d2a87aa060
        This has 6 part, atleast in intro has Chess as example to use RL
    - https://arxiv.org/pdf/1712.01815.pdf (noch nicht angeschaut 06.01.24)
